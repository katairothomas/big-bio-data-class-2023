{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Process:-\n",
    "\n",
    "Credit to: `Jason Brownlee`\n",
    "\n",
    "Instructor: `Mugume Twinamatsiko Atwine`\n",
    "\n",
    "Prerequisite: Numpy, Pandas, Matplotlib\n",
    "\n",
    "Duration: 2Hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import pandas as pd #this is for dataframe manipulation\n",
    "import numpy as np #this is for numerical / mathematical computing\n",
    "import matplotlib.pyplot as plt #this is for visualisation\n",
    "\n",
    "#let's remove the annoying warnings from our cells.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's read in the data\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "data.head(5) #show the top 5 rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions of Your Data\n",
    "You must have a very good handle on how much data you have, both in terms of rows and columns.\n",
    "\n",
    "- `Data Integrity:` The first and foremost reason for checking the dimensions of data is to ensure that the data is complete and accurate. The shape of the data (number of rows and columns) can give an idea of how much data is available for analysis, and can help detect any inconsistencies or missing values in the data.\n",
    "\n",
    "- `Data Preprocessing:` The dimensions of data are important to determine the appropriate preprocessing steps to apply before building a machine learning model. For example, if there are too many missing values or too few data points, certain data cleaning or feature engineering techniques may need to be applied to prepare the data for analysis.\n",
    "\n",
    "- `Model Training:` The dimensions of data also impact the training of machine learning models. Machine learning models require a certain amount of data to learn and generalize patterns in the data. If the dataset is too small, the model may not learn the underlying patterns in the data, and if the dataset is too large, it may lead to overfitting. Thus, understanding the dimensions of the data is crucial for selecting an appropriate model and tuning its hyperparameters.\n",
    "\n",
    "\n",
    "- `Training time estimation:`  the dimensions of data can have a significant impact on the training times of machine learning models. In general, as the size of the dataset (i.e., number of rows and columns) increases, the training time of machine learning models also increases.\n",
    "This is because most machine learning algorithms require multiple passes over the data to learn the underlying patterns and relationships in the data. As the dataset size increases, the number of calculations required to process the data increases, which can result in longer training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to check the dimensions we use the shape function from pandas\n",
    "data.shape\n",
    "#so we see that we have a few rows and columns so its easy to use pandas\n",
    "#if not then we would use something like dask, or partition the data to\n",
    "#to be read in batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "\n",
    "It is important to know your data types for a couple of reasons:-\n",
    "\n",
    "- `Data consistency:` Checking the data types ensures that the data is consistent and in the correct format. Inconsistent data types can lead to errors or incorrect predictions during model training and testing. For example, when you keep the dates they should be in one format such as \"mm/dd/yyyy\" if they shift around you will have errors in analysis\n",
    "\n",
    "- `Feature engineering:` Data types can determine how features are engineered for the model. For example, categorical data requires different feature engineering methods than numerical data. You will be able to plan properly\n",
    "\n",
    "- `Memory optimization:` Checking data types can help optimize memory usage during preprocessing, especially when dealing with large datasets. Data types can be converted to lower memory-consuming types like integers or floats.\n",
    "\n",
    "- `Efficient data manipulation`: Different data types can require different manipulation techniques, and knowing the data types beforehand can save time and improve efficiency during data manipulation.\n",
    "\n",
    "- `Model performance:` Checking the data types can help to identify potential issues that can affect model performance, such as non-numeric data types that require encoding or missing values that require imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                   int64\n",
       "Glucose                       int64\n",
       "BloodPressure                 int64\n",
       "SkinThickness                 int64\n",
       "Insulin                       int64\n",
       "BMI                         float64\n",
       "DiabetesPedigreeFunction    float64\n",
       "Age                           int64\n",
       "Outcome                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we do that by using dtypes\n",
    "data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n",
    "\n",
    "There are several important reasons why you would want to check the descriptive statistics of your data in a machine learning experiment. Here are some of the key reasons:\n",
    "\n",
    "- `Identifying missing or erroneous data:` Descriptive statistics can help you identify if any of your data is missing or if there are any errors in your dataset. For example, you may notice that certain features have a lower count of non-null values than others, which could indicate missing data.\n",
    "\n",
    "- `Understanding the distribution of your data:` Descriptive statistics can give you insights into the distribution of your data, such as the mean, median, and mode. This information can be useful for choosing the appropriate machine learning algorithms, as some algorithms work better with certain types of data distributions.\n",
    "\n",
    "- `Detecting outliers:` Descriptive statistics can also help you detect outliers, which are data points that fall outside the expected range of values. Outliers can have a significant impact on your machine learning model's performance, so it's important to identify and handle them appropriately.\n",
    "\n",
    "- `Assessing the quality of your data:` By examining the descriptive statistics of your data, you can get a sense of the overall quality of your dataset. For example, if you notice that some features have a large range of values compared to others, this could indicate that the data is noisy or that some features may be more important than others.\n",
    "\n",
    "- Descriptive statistics `can give you great insight into the shape of each attribute`. Often you can create more summaries than you have time to review. The describe() function on the Pandas\n",
    "\n",
    "DataFrame lists 8 statistical properties of each attribute. They are:\n",
    "\n",
    "- Count.\n",
    "- Mean.\n",
    "- Standard Deviation.\n",
    "- Minimum Value.\n",
    "- 25th Percentile.\n",
    "- 50th Percentile (Median).\n",
    "- 75th Percentile.\n",
    "- Maximum Value.\n",
    "\n",
    "\n",
    "## Pandas Profiling\n",
    "\n",
    "- This can be made easier using a library called pandasprofiling, this library helps you draw all the graphs but you will still have to make decisions on those graphs to move forward.\n",
    "\n",
    "\n",
    "# Homework:\n",
    ">> Does it make sense to have BloodPressure of 0 in a person as you see in the data.describe below? How would you handle it?\n",
    "What methods would you use?\n",
    "\n",
    "- Install pandasprofiling\n",
    "- Use it to create an EDA (Exploratory Data Analysis Report)\n",
    "- Find a way to replace the data that is 0.00 (and doesnt make sense : mean : numpy, find all 0.0 values and replace them)\n",
    "- Clean your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-profiling\n",
      "  Downloading pandas_profiling-3.6.6-py2.py3-none-any.whl (324 kB)\n",
      "     ------------------------------------ 324.4/324.4 kB 608.8 kB/s eta 0:00:00\n",
      "Collecting ydata-profiling\n",
      "  Using cached ydata_profiling-4.1.2-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: tqdm<4.65,>=4.48.2 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (4.64.1)\n",
      "Collecting multimethod<1.10,>=1.4\n",
      "  Using cached multimethod-1.9.1-py3-none-any.whl (10 kB)\n",
      "Collecting typeguard<2.14,>=2.13.2\n",
      "  Using cached typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Collecting imagehash==4.3.1\n",
      "  Using cached ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (6.0)\n",
      "Collecting phik<0.13,>=0.11.1\n",
      "  Downloading phik-0.12.3-cp39-cp39-win_amd64.whl (663 kB)\n",
      "     ------------------------------------ 663.5/663.5 kB 449.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy<1.10,>=1.4.1 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (1.9.1)\n",
      "Requirement already satisfied: statsmodels<0.14,>=0.13.2 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (0.13.2)\n",
      "Requirement already satisfied: matplotlib<3.7,>=3.2 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (3.5.2)\n",
      "Requirement already satisfied: requests<2.29,>=2.24.0 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (2.28.1)\n",
      "Collecting visions[type_image_path]==0.7.5\n",
      "  Using cached visions-0.7.5-py3-none-any.whl (102 kB)\n",
      "Requirement already satisfied: numpy<1.24,>=1.16.0 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (1.21.5)\n",
      "Requirement already satisfied: seaborn<0.13,>=0.10.1 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (0.11.2)\n",
      "Collecting htmlmin==0.1.12\n",
      "  Using cached htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (2.11.3)\n",
      "Requirement already satisfied: pandas!=1.4.0,<1.6,>1.1 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from ydata-profiling->pandas-profiling) (1.4.4)\n",
      "Collecting pydantic<1.11,>=1.8.1\n",
      "  Downloading pydantic-1.10.7-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyWavelets in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling) (1.3.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling) (9.2.0)\n",
      "Collecting tangled-up-in-unicode>=0.0.4\n",
      "  Using cached tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
      "Requirement already satisfied: attrs>=19.3.0 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (21.4.0)\n",
      "Requirement already satisfied: networkx>=2.4 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (2.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling->pandas-profiling) (2.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from pandas!=1.4.0,<1.6,>1.1->ydata-profiling->pandas-profiling) (2022.1)\n",
      "Requirement already satisfied: joblib>=0.14.1 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from phik<0.13,>=0.11.1->ydata-profiling->pandas-profiling) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from pydantic<1.11,>=1.8.1->ydata-profiling->pandas-profiling) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (3.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from statsmodels<0.14,>=0.13.2->ydata-profiling->pandas-profiling) (0.5.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from tqdm<4.65,>=4.48.2->ydata-profiling->pandas-profiling) (0.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\thomas katairo\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels<0.14,>=0.13.2->ydata-profiling->pandas-profiling) (1.16.0)\n",
      "Building wheels for collected packages: htmlmin\n",
      "  Building wheel for htmlmin (setup.py): started\n",
      "  Building wheel for htmlmin (setup.py): finished with status 'done'\n",
      "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27082 sha256=1e4522336423334681dced1e11493a91af54b8645cb659f6a21d8cec82ca4871\n",
      "  Stored in directory: c:\\users\\thomas katairo\\appdata\\local\\pip\\cache\\wheels\\1d\\05\\04\\c6d7d3b66539d9e659ac6dfe81e2d0fd4c1a8316cc5a403300\n",
      "Successfully built htmlmin\n",
      "Installing collected packages: htmlmin, typeguard, tangled-up-in-unicode, pydantic, multimethod, imagehash, visions, phik, ydata-profiling, pandas-profiling\n",
      "Successfully installed htmlmin-0.1.12 imagehash-4.3.1 multimethod-1.9.1 pandas-profiling-3.6.6 phik-0.12.3 pydantic-1.10.7 tangled-up-in-unicode-0.2.0 typeguard-2.13.3 visions-0.7.5 ydata-profiling-4.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5012fbfd79743b3b43f101357059110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c6dfb3e72048f2a3eaa4274867a1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8471d3acac6141368ee77bfaf0ae7b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d263df5369d34a3089b03585da417c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas_profiling as pp\n",
    "profile = pp.ProfileReport(data)\n",
    "profile.to_file('data.to_html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.6867627785059\n",
      "72.40518417462484\n",
      "29.153419593345657\n",
      "32.45746367239099\n",
      "125.0\n"
     ]
    }
   ],
   "source": [
    "# # Replace 0s in Glucose, BP, skin thickness and BMI with mean because they are normally distributed\n",
    "# mean_glu = data.loc[data['Glucose'] !=0, 'Glucose'].mean()\n",
    "# print(mean_glu)\n",
    "# data['Glucose']= data['Glucose'].replace(0, mean_glu)\n",
    "\n",
    "# mean_bp = data.loc[data['BloodPressure'] !=0, 'BloodPressure'].mean()\n",
    "# print(mean_bp)\n",
    "# data['BloodPressure']= data['BloodPressure'].replace(0, mean_bp)\n",
    "\n",
    "# mean_skin = data.loc[data['SkinThickness'] !=0, 'SkinThickness'].mean()\n",
    "# print(mean_skin)\n",
    "# data['SkinThickness']= data['SkinThickness'].replace(0, mean_skin)\n",
    "\n",
    "# mean_bmi = data.loc[data['BMI'] !=0, 'BMI'].mean()\n",
    "# print(mean_bmi)\n",
    "# data['BMI']= data['BMI'].replace(0, mean_bmi)\n",
    "\n",
    "# median_ins = data.loc[data['Insulin'] !=0, 'Insulin'].median()\n",
    "# print(median_ins)\n",
    "# data['Insulin']= data['Insulin'].replace(0, median_ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6910a787ff446e8c15f867c4b3f098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc592c08f195472986ca464dffc65d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40bb33aa11043e3b16a997c8f162beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274a5e406c8e4f8b950377fbe42043c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profileclean = pp.ProfileReport(data)\n",
    "profileclean.to_file('data.to_html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for this we use the describe function on the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>7</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.305</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>75</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>64</td>\n",
       "      <td>23</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.731</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>6</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.189</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>5</td>\n",
       "      <td>136</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.640</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  \\\n",
       "9              8      125             96              0        0  0.0   \n",
       "49             7      105              0              0        0  0.0   \n",
       "60             2       84              0              0        0  0.0   \n",
       "81             2       74              0              0        0  0.0   \n",
       "145            0      102             75             23        0  0.0   \n",
       "371            0      118             64             23       89  0.0   \n",
       "426            0       94              0              0        0  0.0   \n",
       "494            3       80              0              0        0  0.0   \n",
       "522            6      114              0              0        0  0.0   \n",
       "684            5      136             82              0        0  0.0   \n",
       "706           10      115              0              0        0  0.0   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "9                       0.232   54        1  \n",
       "49                      0.305   24        0  \n",
       "60                      0.304   21        0  \n",
       "81                      0.102   22        0  \n",
       "145                     0.572   21        0  \n",
       "371                     1.731   21        0  \n",
       "426                     0.256   25        0  \n",
       "494                     0.174   22        0  \n",
       "522                     0.189   26        0  \n",
       "684                     0.640   69        0  \n",
       "706                     0.261   30        1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#- create a mask\n",
    "#- use the mask to split the data\n",
    "mask = data.BMI==0\n",
    "\n",
    "data[mask==True]\n",
    "# replace these with the mean."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Distribution (Classiffication Only)\n",
    "\n",
    "Why would someone check the label distribution in an ML/AI experiment?\n",
    "\n",
    "- `Class imbalance:` In some datasets, one class may be significantly more prevalent than the others. This can lead to a class imbalance problem, where the model is biased towards the majority class and performs poorly on the minority class. Checking the label distribution can help identify if there is a class imbalance problem in the data.\n",
    "\n",
    "- `Model performance:` The label distribution can impact the model's performance. If the model is trained on a dataset with a skewed label distribution, it may not generalize well to unseen data. Checking the label distribution can help ensure that the model is trained on a representative sample of the data.\n",
    "\n",
    "- `Data preprocessing:` Depending on the label distribution, different preprocessing techniques may be needed. For example, in a dataset with imbalanced labels, oversampling or undersampling techniques may be needed to balance the classes. Checking the label distribution can help determine what preprocessing techniques are needed.\n",
    "\n",
    "- `Data collection:` The label distribution can provide insights into the data collection process. For example, if a certain label is overrepresented, it may indicate that the data was collected in a biased way. Checking the label distribution can help identify potential biases in the data collection process.\n",
    "\n",
    "- `Domain knowledge:` Checking the label distribution can help provide insights into the problem domain. For example, if a certain label is underrepresented, it may indicate that the problem is rare or difficult to identify. Checking the label distribution can help the modeler understand the problem domain better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the distribution of the class\n",
    "#this only works if we are working with a classification problem not regression\n",
    "#in this case we would have to use methods like smote to deal with the imbalance\n",
    "data.groupby('Outcome').size().plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "Correlation refers to the relationship between two variables and how they may or may not change together. The most common method for calculating correlation is Pearson's Correlation Coefficient, that assumes a normal distribution of the attributes involved. \n",
    "\n",
    "`A correlation of -1 or 1 shows a full negative or positive correlation respectively.` \n",
    "\n",
    "Whereas a value of 0 shows no correlation at all. Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in your dataset. `It is important to know the assumptions of the model you are using before using it`\n",
    "\n",
    "---\n",
    "\n",
    "### Model Assumptions:\n",
    "\n",
    "The supporting reasons for checking the assumptions of a model in an ML experiment, and how it affects how you handle data in the case of a linear regression model.\n",
    "\n",
    "- `Ensures validity of results:` Checking the assumptions of a linear regression model ensures the validity of the results. If the assumptions are not met, the results obtained from the model may not be reliable or valid. For example, if the assumption of linearity is not met, the results obtained from the model may not accurately reflect the relationship between the dependent and independent variables.\n",
    "\n",
    "- `Helps in selecting appropriate variables:` Checking the assumptions of a linear regression model helps in selecting appropriate variables for the model. For example, if the assumption of normality is not met, it may indicate that some variables may not be appropriate for the model or may need to be transformed.\n",
    "\n",
    "- `Improves model accuracy:` Checking the assumptions of a linear regression model can help in improving the accuracy of the model. For example, if the assumption of homoscedasticity is not met, it may indicate that the model needs to be transformed or that a different model may be more appropriate.\n",
    "\n",
    "- `Guides data preparation:` Checking the assumptions of a linear regression model guides data preparation. For example, if the assumption of independence is not met, it may indicate that the data needs to be collected differently or that some observations need to be excluded from the analysis.\n",
    "\n",
    "- `Provides insights into underlying data:` Checking the assumptions of a linear regression model provides insights into the underlying data. For example, if the assumption of normality is not met, it may indicate that there are outliers or that the data is skewed.\n",
    "\n",
    "#### `Comparison of Assumptions between Linear Regression and Logistic Regression`\n",
    "---\n",
    "\n",
    "`Assumptions of Linear Regression Model:`\n",
    "\n",
    "- Linearity: The relationship between the dependent variable and independent variable is linear.\n",
    "- Independence: The observations are independent of each other.\n",
    "- Homoscedasticity: The variance of the errors is constant across all levels of the independent variable.\n",
    "- Normality: The errors follow a normal distribution.\n",
    "- No multicollinearity: There is no high correlation among the independent variables.\n",
    "\n",
    "`Assumptions of Logistic Regression Model:`\n",
    "\n",
    "- Linearity of the logit: The relationship between the independent variable and the logit of the dependent variable is linear.\n",
    "- Independence: The observations are independent of each other.\n",
    "- No multicollinearity: There is no high correlation among the independent variables.\n",
    "- Large sample size: There are a sufficient number of observations for each combination of the independent variables.\n",
    "- No outliers: There are no extreme observations that can influence the results.\n",
    "\n",
    "---\n",
    "\n",
    "As such, it is a good idea to review all of the pairwise correlations of the attributes in your dataset. You can use the corr() function on the Pandas DataFrame to calculate a correlation matrix.\n",
    "\n",
    "There are several reasons why one would consider the correlation between variables while doing machine learning:\n",
    "\n",
    "- `Identify redundant features:` Correlated features can be redundant and provide similar information. Including highly correlated features in a model can lead to overfitting and decrease the model's performance. Identifying and removing these features can help reduce the complexity of the model and improve its accuracy.\n",
    "\n",
    ">> Suppose you are building a model to predict the price of a house based on its size, number of bedrooms, number of bathrooms, and location. The number of bedrooms and number of bathrooms are highly correlated, and including both features in the model can lead to overfitting. By identifying the correlation between these two features and removing one of them, we can reduce the complexity of the model.\n",
    "\n",
    "- `Improve model interpretability:` Understanding the correlation between variables can help to explain the relationships between the features and the target variable. This understanding can improve the interpretability of the model and help to identify important features that contribute most to the prediction.\n",
    "\n",
    ">> Consider a model that predicts whether a customer will buy a product based on their age, income, and education level. By analyzing the correlation between these features and the target variable, we can determine that income has the strongest correlation with the target variable. This understanding can help to explain why income is a critical factor in determining whether a customer buys a product, and improve the interpretability of the model.\n",
    "\n",
    "- `Data preprocessing:` Correlation analysis can be used to identify and remove outliers and missing values in the dataset. This process can help to improve the quality of the data and the model's performance.\n",
    "\n",
    ">> Suppose you are building a model to predict the likelihood of a customer to churn (i.e., leave) a subscription-based service. The dataset includes missing values and outliers, which can affect the quality of the data and the model's performance. By analyzing the correlation between the features and the target variable, we can identify and remove outliers and fill in missing values, which can help to improve the quality of the data and the model's performance.\n",
    "\n",
    "- `Feature engineering:` Correlation analysis can help in creating new features that can improve the model's performance. For example, we can create a new feature by combining highly correlated features or by performing operations on correlated features to create a new feature that is more predictive.\n",
    "\n",
    ">> Consider a model that predicts whether a student will pass a test based on their study hours, attendance rate, and average grade. By analyzing the correlation between these features, we can create a new feature by multiplying the study hours and attendance rate, which are highly correlated. This new feature can be more predictive than either feature alone, and can help to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129459</td>\n",
       "      <td>0.141282</td>\n",
       "      <td>-0.081672</td>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.544341</td>\n",
       "      <td>0.221898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>0.129459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.221071</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.466581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>0.141282</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>0.088933</td>\n",
       "      <td>0.281805</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>0.239528</td>\n",
       "      <td>0.065068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>-0.081672</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.436783</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>0.183928</td>\n",
       "      <td>-0.113970</td>\n",
       "      <td>0.074752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.088933</td>\n",
       "      <td>0.436783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.197859</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>-0.042163</td>\n",
       "      <td>0.130548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.221071</td>\n",
       "      <td>0.281805</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>0.197859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.292695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>0.183928</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>0.173844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.544341</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.239528</td>\n",
       "      <td>-0.113970</td>\n",
       "      <td>-0.042163</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.238356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Outcome</th>\n",
       "      <td>0.221898</td>\n",
       "      <td>0.466581</td>\n",
       "      <td>0.065068</td>\n",
       "      <td>0.074752</td>\n",
       "      <td>0.130548</td>\n",
       "      <td>0.292695</td>\n",
       "      <td>0.173844</td>\n",
       "      <td>0.238356</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Pregnancies   Glucose  BloodPressure  SkinThickness  \\\n",
       "Pregnancies                  1.000000  0.129459       0.141282      -0.081672   \n",
       "Glucose                      0.129459  1.000000       0.152590       0.057328   \n",
       "BloodPressure                0.141282  0.152590       1.000000       0.207371   \n",
       "SkinThickness               -0.081672  0.057328       0.207371       1.000000   \n",
       "Insulin                     -0.073535  0.331357       0.088933       0.436783   \n",
       "BMI                          0.017683  0.221071       0.281805       0.392573   \n",
       "DiabetesPedigreeFunction    -0.033523  0.137337       0.041265       0.183928   \n",
       "Age                          0.544341  0.263514       0.239528      -0.113970   \n",
       "Outcome                      0.221898  0.466581       0.065068       0.074752   \n",
       "\n",
       "                           Insulin       BMI  DiabetesPedigreeFunction  \\\n",
       "Pregnancies              -0.073535  0.017683                 -0.033523   \n",
       "Glucose                   0.331357  0.221071                  0.137337   \n",
       "BloodPressure             0.088933  0.281805                  0.041265   \n",
       "SkinThickness             0.436783  0.392573                  0.183928   \n",
       "Insulin                   1.000000  0.197859                  0.185071   \n",
       "BMI                       0.197859  1.000000                  0.140647   \n",
       "DiabetesPedigreeFunction  0.185071  0.140647                  1.000000   \n",
       "Age                      -0.042163  0.036242                  0.033561   \n",
       "Outcome                   0.130548  0.292695                  0.173844   \n",
       "\n",
       "                               Age   Outcome  \n",
       "Pregnancies               0.544341  0.221898  \n",
       "Glucose                   0.263514  0.466581  \n",
       "BloodPressure             0.239528  0.065068  \n",
       "SkinThickness            -0.113970  0.074752  \n",
       "Insulin                  -0.042163  0.130548  \n",
       "BMI                       0.036242  0.292695  \n",
       "DiabetesPedigreeFunction  0.033561  0.173844  \n",
       "Age                       1.000000  0.238356  \n",
       "Outcome                   0.238356  1.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us plot a heat map to show us the correlation of the data\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(data.corr(method='pearson'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Feature and Label Correlation\n",
    "`Feature selection:` Correlation analysis can help identify which variables are strongly correlated with the label and, therefore, are potentially good features for use in the model. Feature selection is important to prevent overfitting and improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 0.221898\n",
       "Glucose                     0.466581\n",
       "BloodPressure               0.065068\n",
       "SkinThickness               0.074752\n",
       "Insulin                     0.130548\n",
       "BMI                         0.292695\n",
       "DiabetesPedigreeFunction    0.173844\n",
       "Age                         0.238356\n",
       "Outcome                     1.000000\n",
       "Name: Outcome, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can also check the correlation in regards to the outcome\n",
    "#this is valuable especially when we are trying to build an ML algo for that outcome\n",
    "data.corr(method='pearson')['Outcome']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Your Data For Machine Learning\n",
    "\n",
    "Many machine learning algorithms make assumptions about your data. It is often a very good\n",
    "idea to prepare your data in such way to best expose the structure of the problem to the machine\n",
    "learning algorithms that you intend to use. In this chapter you will discover how to prepare\n",
    "your data for machine learning in Python using scikit-learn. After completing this lesson you\n",
    "will know how to:\n",
    "\n",
    "1. Rescale data.\n",
    "2. Standardize data.\n",
    "3. Normalize data.\n",
    "4. Binarize data.\n",
    "\n",
    "\n",
    "### Need For Data Pre-processing\n",
    "You almost always need to pre-process your data. It is a required step. A difficulty is that\n",
    "different algorithms make different assumptions about your data and may require different\n",
    "transforms. Further, when you follow all of the rules and prepare your data, sometimes algorithms\n",
    "can deliver better results without pre-processing.\n",
    "Generally, I would recommend creating many different views and transforms of your data,\n",
    "then exercise a handful of algorithms on each view of your dataset. This will help you to \n",
    "ush\n",
    "out which data transforms might be better at exposing the structure of your problem in general.\n",
    "\n",
    "The steps involved are as below:\n",
    "\n",
    "- Split the dataset into the input and output variables for machine learning.\n",
    "- Apply a pre-processing transform to the input variables.\n",
    "- Summarize the data to show the change.\n",
    "\n",
    "The scikit-learn library provides two standard idioms for transforming data. Each are useful\n",
    "in different circumstances. The transforms are calculated in such a way that they can be applied\n",
    "to your training data and any samples of data you may have in the future. The scikit-learn\n",
    "documentation has some information on how to use various different pre-processing methods:\n",
    "\n",
    "The Fit and Multiple Transform method is the preferred approach. You call the fit()\n",
    "function to prepare the parameters of the transform once on your data. Then later you can use\n",
    "the transform() function on the same data to prepare it for modeling and again on the test or\n",
    "validation dataset or new data that you may see in the future. The Combined Fit-And-Transform\n",
    "is a convenience that you can use for one of the tasks. This might be useful if you are interested\n",
    "in plotting or summarizing the transformed data.\n",
    "\n",
    "\n",
    "## Rescale Data\n",
    "\n",
    "In machine learning, `rescaling refers to the process of transforming the values of a variable to a new scale`, typically to a specific range or distribution. Rescaling is often used as a data preprocessing step to prepare the data for use in a machine learning model.\n",
    "\n",
    "Rescaling can be achieved using various techniques, including normalization and standardization, as well as other methods such as `min-max scaling` and `log transformation`. The choice of rescaling technique depends on the nature of the data and the specific requirements of the model.\n",
    "\n",
    "Rescaling is important in machine learning because it can help to improve the performance of the model by reducing the impact of features that have large values or different scales. Rescaling can also help to simplify the interpretation of the model by making the features more comparable and understandable.\n",
    "\n",
    "- `Avoiding numerical instability:` In some machine learning algorithms, such as gradient-based optimization algorithms, the scale of the input features can affect the convergence of the algorithm. Rescaling the data can help avoid numerical instability and improve the convergence of the algorithm.\n",
    "\n",
    "- `Improving performance:` Rescaling the data can improve the performance of the model, especially when using distance-based algorithms or when the features have different scales. In distance-based algorithms, the distance between two data points is affected by the scale of the features, so rescaling the data can make the distance more meaningful. Similarly, when the features have different scales, rescaling can help give equal importance to all features.\n",
    "\n",
    "- `Reducing computational complexity:` Rescaling the data can reduce the computational complexity of some machine learning algorithms, such as support vector machines. When the features have different scales, some algorithms may require more computation to converge, and rescaling can help reduce this complexity.\n",
    "\n",
    "- `Normalization:` As mentioned in the previous answer, rescaling can be used for normalization, which is a technique used to rescale data to a specific range, typically between 0 and 1. Normalization can help in comparing variables that have different units or scales or when we want to give more importance to small values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.353 0.744 0.59  0.354 0.    0.501 0.234 0.483]\n",
      " [0.059 0.427 0.541 0.293 0.    0.396 0.117 0.167]\n",
      " [0.471 0.92  0.525 0.    0.    0.347 0.254 0.183]\n",
      " [0.059 0.447 0.541 0.232 0.111 0.419 0.038 0.   ]\n",
      " [0.    0.688 0.328 0.354 0.199 0.642 0.944 0.2  ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "array = data.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data\n",
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and\n",
    "differing means and standard deviations to a standard Gaussian distribution with a mean of\n",
    "0 and a standard deviation of 1. \n",
    "\n",
    "`Standardization`, also known as `z-score normalization`, is a technique used to `transform data so that it has zero mean and unit variance`. This is achieved by `subtracting the mean of the data and dividing by its standard deviation.` The resulting transformed data has `a mean of 0` and a `standard deviation of 1`. Standardization is often used when the distribution of the data is approximately normal or when we want to give equal importance to all features.\n",
    "\n",
    "__`It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis.`__ \n",
    "\n",
    "Example in Skearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64   0.848  0.15   0.907 -0.693  0.204  0.468  1.426]\n",
      " [-0.845 -1.123 -0.161  0.531 -0.693 -0.684 -0.365 -0.191]\n",
      " [ 1.234  1.944 -0.264 -1.288 -0.693 -1.103  0.604 -0.106]\n",
      " [-0.845 -0.998 -0.161  0.155  0.123 -0.494 -0.921 -1.042]\n",
      " [-1.142  0.504 -1.505  0.907  0.766  1.41   5.485 -0.02 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "array2 = data.values\n",
    "# separate array into input and output components\n",
    "X = array2[:,0:8]\n",
    "Y = array2[:,8]\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data\n",
    "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called\n",
    "a unit norm or a vector with the length of 1 in linear algebra).\n",
    "\n",
    "`Normalization`, on the other hand, is `a technique used to rescale data to a specific range, typically between 0 and 1.` This is achieved by `subtracting the minimum value of the data and dividing by the range` (i.e., the difference between the maximum and minimum values). Normalization is often used when we want to compare variables that have different units or scales or when we want to give more importance to small values.\n",
    "\n",
    "__`This pre-processing method can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that weight input values such as neural networks and algorithms that use distance measures such as k-Nearest Neighbors.`__ \n",
    "\n",
    "Example in Skearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.034 0.828 0.403 0.196 0.    0.188 0.004 0.28 ]\n",
      " [0.008 0.716 0.556 0.244 0.    0.224 0.003 0.261]\n",
      " [0.04  0.924 0.323 0.    0.    0.118 0.003 0.162]\n",
      " [0.007 0.588 0.436 0.152 0.622 0.186 0.001 0.139]\n",
      " [0.    0.596 0.174 0.152 0.731 0.188 0.01  0.144]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "array3 = data.values\n",
    "# separate array into input and output components\n",
    "X = array3[:,0:8]\n",
    "Y = array3[:,8]\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])\n",
    "print(type(normalizedX))#print the data type so we can know what we are \n",
    "#working with in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Spot-Checking\n",
    "\n",
    "Algorithm spot-checking is a process in machine learning where multiple algorithms are tested and compared on a specific dataset to identify the one that performs the best for a given task. It involves evaluating the performance of various algorithms using a set of standard evaluation metrics, such as accuracy, precision, recall, F1 score, and ROC-AUC score, among others. The goal of algorithm spot-checking is to find the best algorithm that can accurately model the underlying pattern in the data and generalize well to new, unseen data. By trying out multiple algorithms and comparing their performance, algorithm spot-checking helps data scientists and machine learning practitioners to identify the best algorithm for their specific task and improve the overall performance of their models.\n",
    "\n",
    "- `Determine the Best Algorithm:` Spot-checking allows you to compare the performance of multiple algorithms on the same dataset and select the best one for your specific task. Different algorithms are suited to different types of problems, and spot-checking helps you find the one that performs the best on your data.\n",
    "\n",
    "- `Save Time:` Trying out multiple algorithms manually can be time-consuming and inefficient. With algorithm spot-checking, you can automate the process of testing different models, which can save you time and effort.\n",
    "\n",
    "- `Improve Accuracy:` By testing multiple algorithms and comparing their performance, you can improve the accuracy of your model. This is because different algorithms have different strengths and weaknesses, and by choosing the best one for your task, you can improve the overall accuracy of your model.\n",
    "\n",
    "- `Reduce Overfitting:` Overfitting occurs when a model is too complex and captures noise instead of the underlying pattern in the data. By comparing the performance of multiple algorithms, you can identify the one that is less prone to overfitting and select it for your model.\n",
    "\n",
    "- `Gain Insights:` Algorithm spot-checking can also provide valuable insights into the characteristics of your data. By analyzing the performance of different algorithms, you can gain a better understanding of the patterns and relationships within your data, which can help you develop better models in the future.\n",
    "\n",
    "\n",
    "\n",
    "#### Algorithms Overview\n",
    "We are going to take a look at six classification algorithms that you can spot-check on your\n",
    "dataset. Starting with two linear machine learning algorithms:\n",
    "- Logistic Regression.\n",
    "- Linear Discriminant Analysis.\n",
    "- Naive Bayes.\n",
    "\n",
    "Then looking at four nonlinear machine learning algorithms:\n",
    "- k-Nearest Neighbors.\n",
    "- Classification and Regression Trees.\n",
    "- Support Vector Machines.\n",
    "\n",
    "### Linear Machine Learning Algorithms\n",
    "This section demonstrates minimal recipes for how to use two linear machine learning algorithms:\n",
    "`logistic regression` and `linear discriminant analysis`.\n",
    "\n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7695146958304853\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "array = data.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064251538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "array = data.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Machine Learning Algorithms\n",
    "\n",
    "### k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7265550239234451\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "array = data.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = KNeighborsClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classiffication and Regression Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6900205058099795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "array = data.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = DecisionTreeClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510252904989747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "array = data.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = SVC()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot-Check Regression Algorithms\n",
    "\n",
    "- Linear Regression.\n",
    "- Ridge Regression.\n",
    "- LASSO Linear Regression.\n",
    "- Elastic Net Regression.\n",
    "\n",
    "Then looking at three nonlinear machine learning algorithms:\n",
    "- k-Nearest Neighbors.\n",
    "- Classification and Regression Trees.\n",
    "- Support Vector Machines.\n",
    "\n",
    "### Linear Regression\n",
    "Linear regression assumes that the input variables have a Gaussian distribution. It is also\n",
    "assumed that input variables are relevant to the output variable and that they are not highly\n",
    "correlated with each other (a problem called collinearity). You can construct a linear regression\n",
    "model using the LinearRegression class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.705255944524865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "filename = 'housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearRegression()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Ridge regression is an extension of linear regression where the loss function is modified to\n",
    "minimize the complexity of the model measured as the sum squared value of the coefficient\n",
    "values (also called the L2-norm). You can construct a ridge regression model by using the Ridge\n",
    "class2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.078246209259305\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "filename = 'housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = Ridge()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Machine Learning Algorithms\n",
    "This section provides examples of how to use three di\u000b\n",
    "\n",
    "erent nonlinear machine learning algorithms\n",
    "for regression in Python with scikit-learn.\n",
    "\n",
    "### K-Nearest Neighbors\n",
    "The k-Nearest Neighbors algorithm (or KNN) locates the k most similar instances in the\n",
    "training dataset for a new data instance. From the k neighbors, a mean or median output\n",
    "variable is taken as the prediction. Of note is the distance metric used (the metric argument).\n",
    "The Minkowski distance is used by default, which is a generalization of both the Euclidean\n",
    "distance (used when all inputs have the same scale) and Manhattan distance (for when the\n",
    "scales of the input variables di\u000b\n",
    "\n",
    "er). You can construct a KNN model for regression using the\n",
    "KNeighborsRegressor class5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-107.28683898039215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "filename = 'housing.csv'\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = KNeighborsRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Machine Learning Algorithms\n",
    "It is important to compare the performance of multiple di\u000b\n",
    "\n",
    "erent machine learning algorithms\n",
    "consistently. In this chapter you will discover how you can create a test harness to compare\n",
    "multiple different machine learning algorithms in Python with scikit-learn. You can use this\n",
    "test harness as a template on your own machine learning problems and add more and different\n",
    "algorithms to compare. After completing this lesson you will know:\n",
    "\n",
    "1. How to formulate an experiment to directly compare machine learning algorithms.\n",
    "2. A reusable template for evaluating the performance of multiple algorithms on one dataset.\n",
    "3. How to report and visualize the results when comparing algorithm performance.\n",
    "\n",
    "\n",
    "### Choose The Best Machine Learning Model\n",
    "When you work on a machine learning project, you often end up with multiple good models\n",
    "to choose from. Each model will have different performance characteristics. Using resampling\n",
    "methods like cross validation, you can get an estimate for how accurate each model may be on\n",
    "unseen data. You need to be able to use these estimates to choose one or two best models from\n",
    "the suite of models that you have created.\n",
    "When you have a new dataset, it is a good idea to visualize the data using different techniques\n",
    "in order to look at the data from di\u000b\n",
    "\n",
    "erent perspectives. The same idea applies to model selection.\n",
    "You should use a number of di\u000b\n",
    "\n",
    "erent ways of looking at the estimated accuracy of your machine\n",
    "learning algorithms in order to choose the one or two algorithm to finalize. A way to do this is\n",
    "to use visualization methods to show the average accuracy, variance and other properties of the\n",
    "distribution of model accuracies. In the next section you will discover exactly how you can do\n",
    "that in Python with scikit-learn.\n",
    "\n",
    "\n",
    "### Compare Machine Learning Algorithms Consistently\n",
    "The key to a fair comparison of machine learning algorithms is ensuring that each algorithm is\n",
    "evaluated in the same way on the same data. You can achieve this by forcing each algorithm to be evaluated on a consistent test harness. In the example below six different classiffication\n",
    "algorithms are compared on a single dataset:\n",
    "\n",
    "- Logistic Regression.\n",
    "- Linear Discriminant Analysis.\n",
    "- k-Nearest Neighbors.\n",
    "- Classiffication and Regression Trees.\n",
    "- Naive Bayes.\n",
    "- Support Vector Machines.\n",
    "\n",
    "The dataset is the Pima Indians onset of diabetes problem. The problem has two classes and\n",
    "eight numeric input variables of varying scales. The 10-fold cross validation procedure is used to\n",
    "evaluate each algorithm, importantly con\f\n",
    "\n",
    "gured with the same random seed to ensure that the\n",
    "same splits to the training data are performed and that each algorithm is evaluated in precisely\n",
    "the same way. Each algorithm is given a short name, useful for summarizing results afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LR', 0.7695146958304853, 0.04841051924567195)\n",
      "('LDA', 0.773462064251538, 0.05159180390446138)\n",
      "('KNN', 0.7265550239234451, 0.06182131406705549)\n",
      "('CART', 0.6821941216678058, 0.0587306826895643)\n",
      "('NB', 0.7551777170198223, 0.04276593954064409)\n",
      "('SVM', 0.6510252904989747, 0.07214083485055327)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEVCAYAAADuAi4fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAbaklEQVR4nO3dfZRcZYHn8e/PSMj6AnZPwqgkkKiBCYMKMy3uCL5kHTCHcY2Os5ioK3iiOLMDzkF3VhTOEOMyMnPWQceJLyiIL0MCsqJxVxeZBZQ4OKYzZtEEkRBf0hMZG9KIDG9J+O0f97bcFNXd1Z3q6qqb3+ecOql7n3vrPk9V51dPPffWU7JNRETU15NmugIRETG9EvQRETWXoI+IqLkEfUREzSXoIyJqLkEfEVFzCfqYFElXSvrv0/TYb5L0jXHKXyFpaDqO3eskvU/Sp2e6HtGdEvTRlKSbJY1IOrRTx7T997ZPq9TBkp7XqeOr8E5JP5D0b5KGJH1R0vM7VYepsv2Xtt820/WI7pSgjyeQtBB4KWDgNR065pM7cZwJfAT4M+CdQD9wDPBl4A9mslIT6ZLnLrpYgj6aeQvwHeBK4MzxNpT03yT9XNIuSW+r9sIlHS7pc5KGJf1U0oWSnlSWnSXp25IulbQbWF2u21iWf6s8xP+T9ICkN1SO+W5JvyiP+9bK+islfUzS18t9vi3pmZI+XH46+aGkE8dox2LgT4GVtm+0/YjtB8tPGZdMsj33Sdoh6SXl+p1lfc9sqOsnJN0g6VeSvinp6Er5R8r97pe0WdJLK2WrJV0r6QuS7gfOKtd9oSyfU5bdW9Zlk6TfLMueLWmDpN2Stkt6e8PjXlO28VeStkoaGO/1j96QoI9m3gL8fXl71WhINJK0DHgX8PvA84CXN2zyUeBw4Dll2VuAt1bKXwzsAI4ALq7uaPtl5d0X2n6a7avL5WeWj3kksApYK6mvsusZwIXAXOAR4Fbgn8vla4G/GaPNrwSGbH93jPJW23Mb8BvAVcB64EUUz82bgb+T9LTK9m8CPlDWbQvF8z1qE3ACxSeLq4AvSppTKV9etucZDftB8eZ8OLCgrMsfAw+VZeuAIeDZwB8BfynplZV9X1PW+xnABuDvxnk+okck6GM/kk4Bjgausb0ZuAt44xibnwF8xvZW2w8C7688zizgDcB7bf/K9k+ADwH/ubL/Ltsftb3X9kO0Zg+wxvYe218DHgCOrZRfZ3uz7YeB64CHbX/O9j7gaqBpj54iEH8+1kFbbM+PbX+mcqwFZV0fsf0N4FGK0B/1v21/y/YjwAXA70laAGD7C7bvLZ+bDwGHNrTzVttftv1Yk+duT9me59neVz4f95ePfQrwHtsP294CfLqhDRttf61sw+eBF471nETvSNBHozOBb9i+p1y+irGHb54N7KwsV+/PBWYDP62s+ylFT7zZ9q261/beyvKDQLWX/K+V+w81Wa5uu9/jAs8a57ittKfxWNge7/i/br/tB4DdFM/p6PDU7ZJ+Kek+ih763Gb7NvF54HpgfTmk9teSDikfe7ftX43Thrsr9x8E5uQcQO9L0MevSfp3FL30l0u6W9LdwHnACyU169n9HJhfWV5QuX8PRc/y6Mq6o4B/qSx309Sp/xeYP86YdCvtmaxfP1/lkE4/sKscj38PxWvRZ/sZwC8BVfYd87krP+283/ZxwEuAV1MMM+0C+iU9vY1tiB6QoI+q1wL7gOMoxodPAJYAt1AERaNrgLdKWiLpKcBfjBaUH/2vAS6W9PTyROO7gC9Moj7/SjEePu1s3wl8DFin4nr92eVJzRWSzm9TexqdLukUSbMpxur/yfZO4OnAXmAYeLKkvwAOa/VBJS2V9PxyuOl+ijeofeVj/yPwwbJtL6A4z9E4xh81k6CPqjMpxtx/Zvvu0RvFCbk3NX6Et/114G+Bm4DtFCc+oTgJCnAu8G8UJ1w3UgwDXTGJ+qwGPlteOXLGFNs0Ge+kaOta4D6K8xOvA75alh9oexpdBVxEMWTzuxQnZ6EYdvk68COKoZWHmdww1zMpTtTeD9wOfJPH35BWAgspevfXARfZvuEA2hA9QPnhkWgXSUuAHwCHNoyjRwNJV1Jc5XPhTNcl6i89+jggkl5XDnP0AX8FfDUhH9FdEvRxoN5BMZZ8F8X4/p/MbHUiolGGbiIiai49+oiImkvQR0TUXII+IqLmEvQRETWXoI+IqLkEfUREzSXoIyJqLkEfEVFzCfqIiJpL0EdE1FyCPiKi5hL0ERE1l6CPiKi5BH1ERM113a+7z5071wsXLpzpakRE9JTNmzffY3tes7KuC/qFCxcyODg409WIiOgpkn46VlmGbiIiai5BHxFRcwn6iIiaS9BHRNRcgj4iouYS9BERNZegj4iouQR9RETNdd0XpqaDpCnva7uNNYmI6LyDIujHC2tJCfOIqLWWhm4kLZN0h6Ttks5vUn6UpJskfU/SbZJOL9cvlPSQpC3l7RPtbkBERIxvwh69pFnAWuBUYAjYJGmD7W2VzS4ErrH9cUnHAV8DFpZld9k+ob3VjoiIVrXSoz8J2G57h+1HgfXA8oZtDBxW3j8c2NW+KkZExIFoJeiPBHZWlofKdVWrgTdLGqLozZ9bKVtUDul8U9JLmx1A0tmSBiUNDg8Pt177iIiYUCtB3+ySlcazlyuBK23PB04HPi/pScDPgaNsnwi8C7hK0mEN+2L7MtsDtgfmzWs6nXJERExRK0E/BCyoLM/niUMzq4BrAGzfCswB5tp+xPa95frNwF3AMQda6YiIaF0rQb8JWCxpkaTZwApgQ8M2PwNeCSBpCUXQD0uaV57MRdJzgMXAjnZVPiIiJjbhVTe290o6B7gemAVcYXurpDXAoO0NwLuBT0k6j2JY5yzblvQyYI2kvcA+4I9t75621hyk8oWwiBiPuu0/+sDAgDv5U4J1/8JU3dsXEQVJm20PNCvLXDcRETWXoI+IqLkEfUREzSXoIyJqLkEfEVFzCfqIiJo7KOajj96W7wlEHJgEfXS9/HBMxIHJ0E1ERM0l6CMiai5BHxFRcwn6iIiaS9BHRLTRunXrOP7445k1axbHH38869atm+kq5aqbiIh2WbduHRdccAGXX345p5xyChs3bmTVqlUArFy5csbqlR59RESbXHzxxVx++eUsXbqUQw45hKVLl3L55Zdz8cUXz2i9Mh99za/DTvtiph1MX3ibNWsWDz/8MIcccsiv1+3Zs4c5c+awb9++aT125qOPiBlje8xbK+W9ZMmSJWzcuHG/dRs3bmTJkiUzVKNCgj4iok0uuOACVq1axU033cSePXu46aabWLVqFRdccMGM1isnY3tEf38/IyMjU9p3Kh+d+/r62L07P+8bMRmjJ1zPPfdcbr/9dpYsWcLFF188oydiIWP0PTPG2+l65nmJTsjr1z4Zo4+IOIgl6CMiai5BHxFRcwn6iIiaaynoJS2TdIek7ZLOb1J+lKSbJH1P0m2STq+Uvbfc7w5Jr2pn5av6+/uRNOlbWcdJ3/r7+6erKRERbTXh5ZWSZgFrgVOBIWCTpA22t1U2uxC4xvbHJR0HfA1YWN5fAfw28GzgHyQdY7vtXxEbGRnp+FUpERG9oJUe/UnAdts7bD8KrAeWN2xj4LDy/uHArvL+cmC97Uds/xjYXj5eRER0SCtBfySws7I8VK6rWg28WdIQRW/+3Ensi6SzJQ1KGhweHm6x6hER0YpWgr7ZGEXjGMlK4Erb84HTgc9LelKL+2L7MtsDtgfmzZvXQpUiIqJVrUyBMAQsqCzP5/GhmVGrgGUAtm+VNAeY2+K+ERExjVrp0W8CFktaJGk2xcnVDQ3b/Ax4JYCkJcAcYLjcboWkQyUtAhYD321X5SOiO+Sqt+42YY/e9l5J5wDXA7OAK2xvlbQGGLS9AXg38ClJ51EMzZzl4hKYrZKuAbYBe4E/nY4rbiJiZuWqt+5Wm0nN6j7pV92PN1W9Us+6y9/nzMukZhERB7EEfUREzSXoIyJqLkEfEVFzCfqIiJpL0EdE1FyCPiKi5hL0ERE118pcNxExjQ7kW5750lC0IkEfMcPGC+t8AzTaIUM3ERE1l6CPiKi5DN30CF90GKw+vLPHi4haSND3CL3//s7PDri6Y4eLiGmUoZuIiJpL0EdE1Fxthm4yhh0R0Vxtgj5j2BERzWXoJiKi5hL0ERE1l6CPrtDf34+kSd+AKe3X398/wy2O6JzajNFHbxsZGen4OZaIg0V69BERNZcefQ/pZC+0r6+vY8eKiOnVUtBLWgZ8BJgFfNr2JQ3llwJLy8WnAEfYfkZZtg/4fln2M9uvaUfFDzZTHdbINLcRMWHQS5oFrAVOBYaATZI22N42uo3t8yrbnwucWHmIh2yf0L4qR0TEZLTSoz8J2G57B4Ck9cByYNsY268ELmpP9SKiF+Sb6d2tlaA/EthZWR4CXtxsQ0lHA4uAGyur50gaBPYCl9j+cpP9zgbOBjjqqKNaq3lEdI18M727tXLVTbMzgGO9oiuAa23vq6w7yvYA8Ebgw5Ke+4QHsy+zPWB7YN68eS1UKSIiWtVK0A8BCyrL84FdY2y7AlhXXWF7V/nvDuBm9h+/j4iIadZK0G8CFktaJGk2RZhvaNxI0rFAH3BrZV2fpEPL+3OBkxl7bD8iIqbBhGP0tvdKOge4nuLyyitsb5W0Bhi0PRr6K4H13n+gbgnwSUmPUbypXFK9WiciIqafuu0a64GBAQ8ODk56v05fL94r16ennt1xvKlKPbvjeL1A0ubyfOgTZAqEiIiaS9BHRNRcgj4iouYS9BERNZegj4iouQR9RETNJegjImouQR8RUXP5hamIaIv8Alr3qlXQ5w+td2U+896WX0DrbrUJ+vyh9bbMZx4xfTJGHxFRcwn6iIiaS9BHdEB/fz+SJn0DprRff3//DLc4ukltxugjutnIyEjHz0FEjEqPPiKi5hL0ERE1l6CPiKi5jNFHREzRgZwL6eQ5mwR9RMQUjRfW3fRlzAzdRETUXII+IqLmEvQRETWXoI+IqLmWgl7SMkl3SNou6fwm5ZdK2lLefiTpvkrZmZLuLG9ntrPyERExsQmvupE0C1gLnAoMAZskbbC9bXQb2+dVtj8XOLG83w9cBAwABjaX+460tRURETGmVnr0JwHbbe+w/SiwHlg+zvYrgXXl/VcBN9jeXYb7DcCyA6nwVBzIpFEREb2ulaA/EthZWR4q1z2BpKOBRcCNk9lX0tmSBiUNDg8Pt1LvSbE95VtERK9rJeibdWvHSsAVwLW2901mX9uX2R6wPTBv3rwWqhQREa1qJeiHgAWV5fnArjG2XcHjwzaT3TciIqZBK0G/CVgsaZGk2RRhvqFxI0nHAn3ArZXV1wOnSeqT1AecVq6LiIgOmfCqG9t7JZ1DEdCzgCtsb5W0Bhi0PRr6K4H1rgxs294t6QMUbxYAa2zvbm8TIiJiPOq2E44DAwMeHByc6WrURjdNrDSeTtczx+sOvVLPqZiB13yz7YFmZflmbEREzSXoIyJqLvPRR9fo5BfU+vr6OnasiJmWoI+uMNWxzDqP8Ua0S4ZuIiJqLkEfEVFzCfqIiHH09/ePO/HhVCdMHOvW39/f9jZkjD4iYhwjIyMd/w5Eu6VHHxFRcwn6iIiaS9BHRNRcxuhrYKIxvfHKcw16Z/iiw2D14Z09XpfI3+fMS9DXQP4zdD+9//7OT2q2umOHG1f+Pmdehm4iImouQR8RUXMJ+oiImkvQR0TUXII+IqLmEvQRETWXoI+IqLkEfUREzSXoIyJqLkEfEVFzCfqIiJprKeglLZN0h6Ttks4fY5szJG2TtFXSVZX1+yRtKW8b2lXxiF4zlV8bmuqtr69vppsbXWTCSc0kzQLWAqcCQ8AmSRtsb6tssxh4L3Cy7RFJR1Qe4iHbJ7S53hE9ZaoTe0nKpGBxwFrp0Z8EbLe9w/ajwHpgecM2bwfW2h4BsP2L9lYzIiKmqpWgPxLYWVkeKtdVHQMcI+nbkr4jaVmlbI6kwXL9a5sdQNLZ5TaDw8PDk2pARESMr5X56Jv9KkDjZ8knA4uBVwDzgVskHW/7PuAo27skPQe4UdL3bd+134PZlwGXAQwMDORzakREG7XSox8CFlSW5wO7mmzzFdt7bP8YuIMi+LG9q/x3B3AzcOIB1jkiIiahlaDfBCyWtEjSbGAF0Hj1zJeBpQCS5lIM5eyQ1Cfp0Mr6k4FtREREx0w4dGN7r6RzgOuBWcAVtrdKWgMM2t5Qlp0maRuwD/hz2/dKegnwSUmPUbypXFK9WiciIqafuu3SrYGBAQ8ODs50NaJH1P3yw7q3rxd0+jWY6vEkbbY90Kws34yNiKi5BH1ERM0l6CMiaq6V6+gjZpTU7KscrZVnfDsiQR89IGEdcWAydBMRUXMJ+oiImkvQR0TUXMboIyLG4YsOg9WHd/Z4bZagj4gYh95/f+e/Gbu6vY+ZoZuIiJpL0EdE1FyCPiKi5hL0ERE1l6CPiKi5BH1ERM0l6CMiai5BHxFRcwn6iIiaS9BHRNRcgj4iouYS9BERNZegj4ioucxeGRExgYl+t7id+vr62v6YLfXoJS2TdIek7ZLOH2ObMyRtk7RV0lWV9WdKurO8ndmuikdEdILtKd2muu/u3bvb3oYJe/SSZgFrgVOBIWCTpA22t1W2WQy8FzjZ9oikI8r1/cBFwABgYHO570jbWxIREU210qM/Cdhue4ftR4H1wPKGbd4OrB0NcNu/KNe/CrjB9u6y7AZgWXuqHhERrWgl6I8EdlaWh8p1VccAx0j6tqTvSFo2iX2RdLakQUmDw8PDrdc+IiIm1ErQNzsL0fi7Wk8GFgOvAFYCn5b0jBb3xfZltgdsD8ybN6+FKkVERKtaCfohYEFleT6wq8k2X7G9x/aPgTsogr+VfSMiYhq1EvSbgMWSFkmaDawANjRs82VgKYCkuRRDOTuA64HTJPVJ6gNOK9dFRESHTHjVje29ks6hCOhZwBW2t0paAwza3sDjgb4N2Af8ue17ASR9gOLNAmCN7fZfOxQREWPS6PWe3WJgYMCDg4MzXY2IriCJbvs/Gq3p9GsnabPtgWZlmQIhIqLmEvQRETWXoI+IqLkEfUREzSXoIyJqLkEfEVFzCfqIiJpL0EdE1FyCPiKi5hL0ERE1l6CPiKi5BH1ERM0l6CMiam7CaYojYnpJzX6IrbXyzGwZrUjQR8ywhHVMtwzdRETUXII+IqLmEvQRETWXoI+IqLkEfUREzSXoIyJqLkEfEVFzuY4+ImKKeuXLbgn6iIgp6pUvu7U0dCNpmaQ7JG2XdH6T8rMkDUvaUt7eVinbV1m/oZ2Vj4iIiU3Yo5c0C1gLnAoMAZskbbC9rWHTq22f0+QhHrJ9woFXNSIipqKVHv1JwHbbO2w/CqwHlk9vtSIiol1aCfojgZ2V5aFyXaPXS7pN0rWSFlTWz5E0KOk7kl57IJWNiIjJayXom502bjwD8VVgoe0XAP8AfLZSdpTtAeCNwIclPfcJB5DOLt8MBoeHh1usekREtKKVoB8Cqj30+cCu6ga277X9SLn4KeB3K2W7yn93ADcDJzYewPZltgdsD8ybN29SDYiIiPG1EvSbgMWSFkmaDawA9rt6RtKzKouvAW4v1/dJOrS8Pxc4GWg8iRsREdNowqtubO+VdA5wPTALuML2VklrgEHbG4B3SnoNsBfYDZxV7r4E+KSkxyjeVC5pcrVORERMI3XbBf+ShoGfdvCQc4F7Oni8Tkv7elva17s63bajbTcd++66oO80SYPlyeJaSvt6W9rXu7qpbZnULCKi5hL0ERE1l6CHy2a6AtMs7ettaV/v6pq2HfRj9BERdZcefUREzR1UQS/pgSbrVkv6l3Ia5W2SVs5E3aaihfbcKelLko5r2GaepD2S3tG52k5OtW2STi/bclTZvgclHTHGtpb0ocryf5W0umMVn4CkZ0paL+mu8u/ta5KOKcvOk/SwpMMr279C0i8lfU/SDyX9j3L9WyvTfz8q6fvl/Utmqm1jGe81afh7/aGkj0vq+lySdIGkreX8XlskfV3SBxu2OUHS6JdHfyLplobyLZJ+0In6dv0T2iGXllMpL6f4gtchM12hA3Sp7RNsLwauBm6UVL2+9j8B3wG6/k1N0iuBjwLLbP+sXH0P8O4xdnkE+MPym9hdRcXPDV0H3Gz7ubaPA94H/Ga5yUqKb6K/rmHXW2yfSDF9yKslnWz7M+VrfALFlCRLy+Un/F5EF5joNRn9/3cc8Hzg5R2r2RRI+j3g1cDvlPN7/T5wCfCGhk1XAFdVlp8+OuGjpCWdqOuoBH2F7TuBB4G+ma5Lu9i+GvgGxaRyo1ZSBOV8Sc1mIu0Kkl5KMXfSH9i+q1J0BfAGSf1NdttLcRLsvA5UcbKWAntsf2J0he0ttm8pJ/t7GnAhY7wB234I2ELz2WO7WauvyWxgDjAy7TU6MM8C7hmd38v2Pba/Cdwn6cWV7c6gmNZ91DU8/mawEljXicpCgn4/kn4HuNP2L2a6Lm32z8BvAZQ9imfa/i77/+F1m0OBrwCvtf3DhrIHKML+z8bYdy3wpuoQSJc4Htg8Rtnof/xbgGOrQ1OjJPUBi4FvTVsNp894r8l5krYAPwd+ZHtLZ6s2ad8AFkj6kaSPSRr9BLKOohePpH8P3Ft2HkddC/xhef8/Usz62xEJ+sJ5ku4A/glYPcN1mQ7VqaZXUAQ8FL2Nbh2+2QP8I7BqjPK/Bc6UdFhjge37gc8B75y+6rXdCmC97ceAL1EMr416qaTbgLuB/2X77pmo4IGY4DUZHbo5AniqpBUdrdwk2X6AYobes4Fh4GpJZ1H8f/qj8hzDCp7YY98NjJTtu51i9KAjEvSFS20fS9G7/ZykOTNdoTY7kXJGUYpgP0vSTyhmIX2hpMUzVbFxPEbx0fdFkt7XWGj7Porxz/8yxv4fpniTeOq01XDytlKZwnuUpBdQ9NRvKF+XFez/BnxLORb8fOBPJPXqT3OO+5rY3gP8H+BlnazUVNjeZ/tm2xcB5wCvt70T+AnFOYbX83iHqupqik83HRu2gQT9fmx/CRgEzpzpurSLpNcDpwHrJB0LPNX2kbYX2l4IfJDy42a3sf0gxUmvN0lq1rP/G+AdNJmF1fZuiv9oY30imAk3AodKevvoCkkvAj4CrB59TWw/GzhS0tHVnW3/iOL1ek8nK90uE70m5cnqlwB3NSvvFpKObegcncDjEzGuAy4F7rI91GT364C/ppgNuGMOtqB/iqShyu1dTbZZA7yrFy7xYuz2nDd6eSXwZuA/2B6m6CVe1/AY/5PuHb4ZDYdlwIWSljeU3UPRnkPH2P1DFDMIdgUX3058HXBqeXnlVoqhwlfwxNflOpq/AX8CeJmkRdNY1enU7DUZHaP/AcWb9sc6XqvJeRrw2fLy2NsorhZaXZZ9Efht9j8J+2u2f2X7r8rf3+6YfDM2IqLmeqHXGhERByBBHxFRcwn6iIiaS9BHRNRcgj4iouYS9BERNZegj4iouQR9RETN/X/9178CfHr8oAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# load dataset\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "\n",
    "#split the dataset \n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# prepare models and add them to a list\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# models.append(('ADA', AdaBoostClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, it would suggest that both logistic regression and linear discriminant analysis are perhaps worthy of further study on this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate Machine Learning Workflows with Pipelines\n",
    "---\n",
    "\n",
    "<img src=\"https://www.wordstream.com/wp-content/uploads/2021/07/machine-learning.png\">\n",
    "\n",
    "---\n",
    "\n",
    "There are standard workflows in a machine learning project that can be automated. In Python\n",
    "scikit-learn, Pipelines help to clearly define and automate these work\n",
    "ows. In this chapter you will discover Pipelines in scikit-learn and how you can automate common machine learning\n",
    "work\n",
    "ows. After completing this lesson you will know:\n",
    "\n",
    "1. How to use pipelines to minimize data leakage.\n",
    "2. How to construct a data preparation and modeling pipeline.\n",
    "3. How to construct a feature extraction and modeling pipeline.\n",
    "\n",
    "### Automating Machine Learning Workflows\n",
    "There are standard work\n",
    "ows in applied machine learning. Standard because they overcome\n",
    "common problems like data leakage in your test harness. Python scikit-learn provides a Pipeline\n",
    "utility to help automate machine learning work\n",
    "ows. Pipelines work by allowing for a linear\n",
    "sequence of data transforms to be chained together culminating in a modeling process that can\n",
    "be evaluated.\n",
    "\n",
    "The goal is to ensure that all of the steps in the pipeline are constrained to the data available\n",
    "for the evaluation, such as the training dataset or each fold of the cross validation procedure.\n",
    "\n",
    "### Data Preparation and Modeling Pipeline\n",
    "An easy trap to fall into in applied machine learning is leaking data from your training dataset\n",
    "to your test dataset. To avoid this trap you need a robust test harness with strong separation of training and testing. This includes data preparation. Data preparation is one easy way to leak\n",
    "knowledge of the whole training dataset to the algorithm. For example, preparing your data\n",
    "using normalization or standardization on the entire training dataset before learning would not\n",
    "be a valid test because the training dataset would have been in\n",
    "uenced by the scale of the data\n",
    "in the test set.\n",
    "\n",
    "\n",
    "Pipelines help you prevent data leakage in your test harness by ensuring that data preparation\n",
    "like standardization is constrained to each fold of your cross validation procedure. The example\n",
    "below demonstrates this important data preparation and model evaluation work\n",
    "ow on the Pima Indians onset of diabetes dataset. The pipeline is defined with two steps:\n",
    "\n",
    "1. Standardize the data.\n",
    "2. Learn a Linear Discriminant Analysis model.\n",
    "\n",
    "The pipeline is then evaluated using 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064251538\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that standardizes the data then creates a model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# load data\n",
    "\n",
    "dataframe = read_csv('diabetes.csv')\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lda', LinearDiscriminantAnalysis()))\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we create a Python list of steps that are provided to the Pipeline for process\n",
    "the data. Also notice how the Pipeline itself is treated like an estimator and is evaluated in its\n",
    "entirety by the k-fold cross validation procedure. Running the example provides a summary of\n",
    "accuracy of the setup on the dataset.\n",
    "\n",
    "\n",
    "## Feature Extraction and Modeling Pipeline\n",
    "Feature extraction is another procedure that is susceptible to data leakage. Like data preparation,\n",
    "feature extraction procedures must be restricted to the data in your training dataset. The\n",
    "pipeline provides a handy tool called the FeatureUnion which allows the results of multiple\n",
    "feature selection and extraction procedures to be combined into a larger dataset on which a\n",
    "model can be trained. Importantly, all the feature extraction and the feature union occurs\n",
    "within each fold of the cross validation procedure. The example below demonstrates the pipeline\n",
    "defined with four steps:\n",
    "\n",
    "1. Feature Extraction with Principal Component Analysis (3 features).\n",
    "2. Feature Extraction with Statistical Selection (6 features).\n",
    "3. Feature Union.\n",
    "4. Learn a Logistic Regression Model.\n",
    "\n",
    "The pipeline is then evaluated using 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7760423786739576\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that extracts features from the data then creates a model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "# load data\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3)))\n",
    "features.append(('select_best', SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve Performance with Ensembles\n",
    "\n",
    "Ensembles can give you a boost in accuracy on your dataset. In this chapter you will discover\n",
    "how you can create some of the most powerful types of ensembles in Python using scikit-learn.\n",
    "This lesson will step you through Boosting, Bagging and Majority Voting and show you how you\n",
    "can continue to ratchet up the accuracy of the models on your own datasets. After completing\n",
    "this lesson you will know:\n",
    "\n",
    "1. How to use bagging ensemble methods such as bagged decision trees, random forest and extra trees.\n",
    "2. How to use boosting ensemble methods such as AdaBoost and stochastic gradient boosting.\n",
    "3. How to use voting ensemble methods to combine the predictions from multiple algorithms.\n",
    "\n",
    "### Combine Models Into Ensemble Predictions\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "- Bagging. Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "- Boosting. Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models.\n",
    "- Voting. Building multiple models (typically of di\u000b\n",
    "\n",
    "ering types) and simple statistics (like calculating the mean) are used to combine predictions.\n",
    "\n",
    "This assumes you are generally familiar with machine learning algorithms and ensemble\n",
    "methods and will not go into the details of how the algorithms work or their parameters.\n",
    "The Pima Indians onset of Diabetes dataset is used to demonstrate each algorithm. Each\n",
    "ensemble algorithm is demonstrated using 10-fold cross validation and the classiffication accuracy\n",
    "performance metric.\n",
    "\n",
    "\n",
    "### Bagging Algorithms\n",
    "Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset\n",
    "(with replacement) and training a model for each sample. The \f\n",
    "\n",
    "nal output prediction is averaged\n",
    "across the predictions of all of the sub-models. The three bagging models covered in this section\n",
    "are as follows:\n",
    "\n",
    "- Bagged Decision Trees.\n",
    "- Random Forest.\n",
    "- Extra Trees.\n",
    "\n",
    "### Bagged Decision Trees\n",
    "Bagging performs best with algorithms that have high variance. A popular example are\n",
    "decision trees, often constructed without pruning. In the example below is an example\n",
    "of using the BaggingClassifier with the Classi\f\n",
    "\n",
    "cation and Regression Trees algorithm\n",
    "(DecisionTreeClassifier1). A total of 100 trees are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770762132604238\n"
     ]
    }
   ],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#let's read the data\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "\n",
    "#split the data in portions\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7 #duplication\n",
    "\n",
    "#split according to cross validation\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "#initialize the model\n",
    "cart = DecisionTreeClassifier()\n",
    "\n",
    "#bagging\n",
    "num_trees = 250\n",
    "\n",
    "#model\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Random Forests is an extension of bagged decision trees. Samples of the training dataset are\n",
    "taken with replacement, but the trees are constructed in a way that reduces the correlation\n",
    "between individual classiffiers. Specifically, rather than greedily choosing the best split point in\n",
    "the construction of each tree, only a random subset of features are considered for each split. You\n",
    "can construct a Random Forest model for classiffication using the RandomForestClassifier\n",
    "class2. The example below demonstrates using Random Forest for classiffication with 100 trees\n",
    "and split points chosen from a random selection of 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681647300068353\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "#let's read the data\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "num_trees = 1000\n",
    "\n",
    "max_features = 3\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees\n",
    "Extra Trees are another modi\f\n",
    "\n",
    "cation of bagging where random trees are constructed from\n",
    "samples of the training dataset. You can construct an Extra Trees model for classiffication using\n",
    "the ExtraTreesClassifier class3. The example below provides a demonstration of extra trees\n",
    "with the number of trees set to 100 and splits chosen from 7 random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7642686261107314\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#let's read the data\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Algorithms\n",
    "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes\n",
    "of the models before them in the sequence. Once created, the models make predictions which\n",
    "may be weighted by their demonstrated accuracy and the results are combined to create a \f\n",
    "\n",
    "nal\n",
    "output prediction. The two most common boosting ensemble machine learning algorithms are:\n",
    "\n",
    "- AdaBoost.\n",
    "- Stochastic Gradient Boosting.\n",
    "\n",
    "\n",
    "### AdaBoost\n",
    "AdaBoost was perhaps the \f\n",
    "\n",
    "rst successful boosting ensemble algorithm. It generally works\n",
    "by weighting instances in the dataset by how easy or di\u000ecult they are to classify, allowing\n",
    "the algorithm to pay or less attention to them in the construction of subsequent models. You\n",
    "can construct an AdaBoost model for classi\f\n",
    "\n",
    "cation using the AdaBoostClassifier class4. The\n",
    "example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760457963089542\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "num_trees = 30\n",
    "seed=7\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting\n",
    "Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most\n",
    "sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of\n",
    "the best techniques available for improving performance via ensembles. You can construct a\n",
    "Gradient Boosting model for classiffication using the GradientBoostingClassifier class5. The\n",
    "example below demonstrates Stochastic Gradient Boosting for classi\f\n",
    "\n",
    "cation with 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7669002050580999\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7668660287081339\n"
     ]
    }
   ],
   "source": [
    "# Stochastic X Gradient Boosting Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = XGBClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Ensemble\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning\n",
    "algorithms. It works by first creating two or more standalone models from your training dataset.\n",
    "A Voting Classiffier can then be used to wrap your models and average the predictions of the\n",
    "sub-models when asked to make predictions for new data. The predictions of the sub-models can\n",
    "be weighted, but specifying the weights for classiffiers manually or even heuristically is difficult.\n",
    "More advanced methods can learn how to best weight the predictions from sub-models, but this\n",
    "is called stacking (stacked aggregation) and is currently not provided in scikit-learn.\n",
    "You can create a voting ensemble model for classiffication using the VotingClassifier\n",
    "class6. The code below provides an example of combining the predictions of logistic regression,\n",
    "classiffication and regression trees and support vector machines together for a classiffication\n",
    "problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7642515379357484\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "\n",
    "model4 = XGBClassifier()\n",
    "estimators.append(('xgb', model4))\n",
    "\n",
    "model5 = RandomForestClassifier()\n",
    "estimators.append(('rfc', model5))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Your Model with pickle\n",
    "Pickle is the standard way of serializing objects in Python. You can use the pickle1 operation\n",
    "to serialize your machine learning algorithms and save the serialized format to a file. Later you\n",
    "can load this file to deserialize your model and use it to make new predictions. The example\n",
    "below demonstrates how you can train a logistic regression model on the Pima Indians onset of\n",
    "diabetes dataset, save the model to file and load it to make predictions on the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7559055118110236\n"
     ]
    }
   ],
   "source": [
    "# Save Model Using Pickle\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "# Fit the model on 33%\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model1.sav'\n",
    "dump(model, open(filename, 'wb'))\n",
    "\n",
    "# some time later...\n",
    "# load the model from disk\n",
    "loaded_model = load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Your Model with Joblib\n",
    "The Joblib2 library is part of the SciPy ecosystem and provides utilities for pipelining Python\n",
    "jobs. It provides utilities for saving and loading Python objects that make use of NumPy data\n",
    "structures, efficiently3. This can be useful for some machine learning algorithms that require a\n",
    "lot of parameters or store the entire dataset (e.g. k-Nearest Neighbors). The example below\n",
    "demonstrates how you can train a logistic regression model on the Pima Indians onset of diabetes\n",
    "dataset, save the model to file using Joblib and load it to make predictions on the unseen test\n",
    "set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7559055118110236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Save Model Using joblib\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals.joblib import dump\n",
    "from sklearn.externals.joblib import load\n",
    "filename = 'diabetes.csv'\n",
    "dataframe = read_csv(filename)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "# Fit the model on 33%\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "dump(model, filename)\n",
    "\n",
    "# some time later...\n",
    "# load the model from disk\n",
    "loaded_model = load(filename)\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Summary\n",
    "Below is the project template that you can use in your machine learning projects in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Project Template\n",
    "# 1. Prepare Problem\n",
    "# a) Load libraries\n",
    "# b) Load dataset\n",
    "\n",
    "# 2. Summarize Data\n",
    "# a) Descriptive statistics\n",
    "# b) Data visualizations\n",
    "\n",
    "# 3. Prepare Data\n",
    "# a) Data Cleaning\n",
    "# b) Feature Selection\n",
    "# c) Data Transforms\n",
    "\n",
    "# 4. Evaluate Algorithms\n",
    "# a) Split-out validation dataset\n",
    "# b) Test options and evaluation metric\n",
    "# c) Spot Check Algorithms\n",
    "# d) Compare Algorithms\n",
    "\n",
    "# 5. Improve Accuracy\n",
    "# a) Algorithm Tuning\n",
    "# b) Ensembles\n",
    "\n",
    "# 6. Finalize Model\n",
    "# a) Predictions on validation dataset\n",
    "# b) Create standalone model on entire training dataset\n",
    "# c) Save model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- --------------------\n",
      "alabaster                     0.7.12\n",
      "anaconda-client               1.11.0\n",
      "anaconda-navigator            2.3.1\n",
      "anaconda-project              0.11.1\n",
      "anyio                         3.5.0\n",
      "appdirs                       1.4.4\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "arrow                         1.2.2\n",
      "astroid                       2.11.7\n",
      "astropy                       5.1\n",
      "atomicwrites                  1.4.0\n",
      "attrs                         21.4.0\n",
      "Automat                       20.2.0\n",
      "autopep8                      1.6.0\n",
      "Babel                         2.9.1\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.4\n",
      "backports.tempfile            1.0\n",
      "backports.weakref             1.0.post1\n",
      "bcrypt                        3.2.0\n",
      "beautifulsoup4                4.11.1\n",
      "binaryornot                   0.4.4\n",
      "biopython                     1.81\n",
      "bitarray                      2.5.1\n",
      "bkcharts                      0.2\n",
      "black                         22.6.0\n",
      "bleach                        4.1.0\n",
      "bokeh                         2.4.3\n",
      "boto3                         1.24.28\n",
      "botocore                      1.27.28\n",
      "Bottleneck                    1.3.5\n",
      "brotlipy                      0.7.0\n",
      "certifi                       2022.9.14\n",
      "cffi                          1.15.1\n",
      "chardet                       4.0.0\n",
      "charset-normalizer            2.0.4\n",
      "click                         8.0.4\n",
      "cloudpickle                   2.0.0\n",
      "clyent                        1.2.2\n",
      "colorama                      0.4.5\n",
      "colorcet                      3.0.0\n",
      "comtypes                      1.1.10\n",
      "conda                         22.9.0\n",
      "conda-build                   3.22.0\n",
      "conda-content-trust           0.1.3\n",
      "conda-pack                    0.6.0\n",
      "conda-package-handling        1.9.0\n",
      "conda-repo-cli                1.0.20\n",
      "conda-token                   0.4.0\n",
      "conda-verify                  3.4.2\n",
      "constantly                    15.1.0\n",
      "cookiecutter                  1.7.3\n",
      "cryptography                  37.0.1\n",
      "cssselect                     1.1.0\n",
      "cycler                        0.11.0\n",
      "Cython                        0.29.32\n",
      "cytoolz                       0.11.0\n",
      "daal4py                       2021.6.0\n",
      "dask                          2022.7.0\n",
      "datashader                    0.14.1\n",
      "datashape                     0.5.4\n",
      "debugpy                       1.5.1\n",
      "decorator                     5.1.1\n",
      "defusedxml                    0.7.1\n",
      "diff-match-patch              20200713\n",
      "dill                          0.3.4\n",
      "distributed                   2022.7.0\n",
      "docutils                      0.18.1\n",
      "entrypoints                   0.4\n",
      "et-xmlfile                    1.1.0\n",
      "fastjsonschema                2.16.2\n",
      "filelock                      3.6.0\n",
      "flake8                        4.0.1\n",
      "Flask                         1.1.2\n",
      "fonttools                     4.25.0\n",
      "fsspec                        2022.7.1\n",
      "future                        0.18.2\n",
      "gensim                        4.1.2\n",
      "glob2                         0.7\n",
      "greenlet                      1.1.1\n",
      "h5py                          3.7.0\n",
      "HeapDict                      1.0.1\n",
      "holoviews                     1.15.0\n",
      "htmlmin                       0.1.12\n",
      "hvplot                        0.8.0\n",
      "hyperlink                     21.0.0\n",
      "idna                          3.3\n",
      "imagecodecs                   2021.8.26\n",
      "ImageHash                     4.3.1\n",
      "imageio                       2.19.3\n",
      "imagesize                     1.4.1\n",
      "importlib-metadata            4.11.3\n",
      "incremental                   21.3.0\n",
      "inflection                    0.5.1\n",
      "iniconfig                     1.1.1\n",
      "intake                        0.6.5\n",
      "intervaltree                  3.1.0\n",
      "ipykernel                     6.15.2\n",
      "ipython                       7.31.1\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    7.6.5\n",
      "isort                         5.9.3\n",
      "itemadapter                   0.3.0\n",
      "itemloaders                   1.0.4\n",
      "itsdangerous                  2.0.1\n",
      "jdcal                         1.4.1\n",
      "jedi                          0.18.1\n",
      "jellyfish                     0.9.0\n",
      "Jinja2                        2.11.3\n",
      "jinja2-time                   0.2.0\n",
      "jmespath                      0.10.0\n",
      "joblib                        1.1.0\n",
      "json5                         0.9.6\n",
      "jsonschema                    4.16.0\n",
      "jupyter                       1.0.0\n",
      "jupyter_client                7.3.4\n",
      "jupyter-console               6.4.3\n",
      "jupyter_core                  4.11.1\n",
      "jupyter-server                1.18.1\n",
      "jupyterlab                    3.4.4\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab-server             2.10.3\n",
      "jupyterlab-widgets            1.0.0\n",
      "keyring                       23.4.0\n",
      "kiwisolver                    1.4.2\n",
      "lazy-object-proxy             1.6.0\n",
      "libarchive-c                  2.9\n",
      "llvmlite                      0.38.0\n",
      "locket                        1.0.0\n",
      "lxml                          4.9.1\n",
      "lz4                           3.1.3\n",
      "Markdown                      3.3.4\n",
      "MarkupSafe                    2.0.1\n",
      "matplotlib                    3.5.2\n",
      "matplotlib-inline             0.1.6\n",
      "mccabe                        0.6.1\n",
      "menuinst                      1.4.19\n",
      "mistune                       0.8.4\n",
      "mkl-fft                       1.3.1\n",
      "mkl-random                    1.2.2\n",
      "mkl-service                   2.4.0\n",
      "mock                          4.0.3\n",
      "mpmath                        1.2.1\n",
      "msgpack                       1.0.3\n",
      "multimethod                   1.9.1\n",
      "multipledispatch              0.6.0\n",
      "munkres                       1.1.4\n",
      "mypy-extensions               0.4.3\n",
      "navigator-updater             0.3.0\n",
      "nbclassic                     0.3.5\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     6.4.4\n",
      "nbformat                      5.5.0\n",
      "nest-asyncio                  1.5.5\n",
      "networkx                      2.8.4\n",
      "nltk                          3.7\n",
      "nose                          1.3.7\n",
      "notebook                      6.4.12\n",
      "numba                         0.55.1\n",
      "numexpr                       2.8.3\n",
      "numpy                         1.21.5\n",
      "numpydoc                      1.4.0\n",
      "olefile                       0.46\n",
      "openpyxl                      3.0.10\n",
      "packaging                     21.3\n",
      "pandas                        1.4.4\n",
      "pandas-profiling              3.6.6\n",
      "pandocfilters                 1.5.0\n",
      "panel                         0.13.1\n",
      "param                         1.12.0\n",
      "paramiko                      2.8.1\n",
      "parsel                        1.6.0\n",
      "parso                         0.8.3\n",
      "partd                         1.2.0\n",
      "pathlib                       1.0.1\n",
      "pathspec                      0.9.0\n",
      "patsy                         0.5.2\n",
      "pep8                          1.7.1\n",
      "pexpect                       4.8.0\n",
      "phik                          0.12.3\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.2.0\n",
      "pip                           23.1.2\n",
      "pkginfo                       1.8.2\n",
      "platformdirs                  2.5.2\n",
      "plotly                        5.9.0\n",
      "pluggy                        1.0.0\n",
      "poyo                          0.5.0\n",
      "prometheus-client             0.14.1\n",
      "prompt-toolkit                3.0.20\n",
      "Protego                       0.1.16\n",
      "psutil                        5.9.0\n",
      "ptyprocess                    0.7.0\n",
      "py                            1.11.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycodestyle                   2.8.0\n",
      "pycosat                       0.6.3\n",
      "pycparser                     2.21\n",
      "pyct                          0.4.8\n",
      "pycurl                        7.45.1\n",
      "pydantic                      1.10.7\n",
      "PyDispatcher                  2.0.5\n",
      "pydocstyle                    6.1.1\n",
      "pyerfa                        2.0.0\n",
      "pyflakes                      2.4.0\n",
      "Pygments                      2.11.2\n",
      "PyHamcrest                    2.0.2\n",
      "PyJWT                         2.4.0\n",
      "pylint                        2.14.5\n",
      "pyls-spyder                   0.4.0\n",
      "PyNaCl                        1.5.0\n",
      "pyodbc                        4.0.34\n",
      "pyOpenSSL                     22.0.0\n",
      "pyparsing                     3.0.9\n",
      "pyrsistent                    0.18.0\n",
      "PySocks                       1.7.1\n",
      "pytest                        7.1.2\n",
      "python-dateutil               2.8.2\n",
      "python-lsp-black              1.0.0\n",
      "python-lsp-jsonrpc            1.0.0\n",
      "python-lsp-server             1.3.3\n",
      "python-slugify                5.0.2\n",
      "python-snappy                 0.6.0\n",
      "pytz                          2022.1\n",
      "pyviz-comms                   2.0.2\n",
      "PyWavelets                    1.3.0\n",
      "pywin32                       302\n",
      "pywin32-ctypes                0.2.0\n",
      "pywinpty                      2.0.2\n",
      "PyYAML                        6.0\n",
      "pyzmq                         23.2.0\n",
      "QDarkStyle                    3.0.2\n",
      "qstylizer                     0.1.10\n",
      "QtAwesome                     1.0.3\n",
      "qtconsole                     5.2.2\n",
      "QtPy                          2.2.0\n",
      "queuelib                      1.5.0\n",
      "regex                         2022.7.9\n",
      "requests                      2.28.1\n",
      "requests-file                 1.5.1\n",
      "rope                          0.22.0\n",
      "Rtree                         0.9.7\n",
      "ruamel-yaml-conda             0.15.100\n",
      "s3transfer                    0.6.0\n",
      "scikit-image                  0.19.2\n",
      "scikit-learn                  1.0.2\n",
      "scikit-learn-intelex          2021.20221004.171935\n",
      "scipy                         1.9.1\n",
      "Scrapy                        2.6.2\n",
      "seaborn                       0.11.2\n",
      "Send2Trash                    1.8.0\n",
      "service-identity              18.1.0\n",
      "setuptools                    63.4.1\n",
      "sip                           4.19.13\n",
      "six                           1.16.0\n",
      "smart-open                    5.2.1\n",
      "sniffio                       1.2.0\n",
      "snowballstemmer               2.2.0\n",
      "sortedcollections             2.1.0\n",
      "sortedcontainers              2.4.0\n",
      "soupsieve                     2.3.1\n",
      "Sphinx                        5.0.2\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.5\n",
      "spyder                        5.2.2\n",
      "spyder-kernels                2.2.1\n",
      "SQLAlchemy                    1.4.39\n",
      "statsmodels                   0.13.2\n",
      "sympy                         1.10.1\n",
      "tables                        3.6.1\n",
      "tabulate                      0.8.10\n",
      "tangled-up-in-unicode         0.2.0\n",
      "TBB                           0.2\n",
      "tblib                         1.7.0\n",
      "tenacity                      8.0.1\n",
      "terminado                     0.13.1\n",
      "testpath                      0.6.0\n",
      "text-unidecode                1.3\n",
      "textdistance                  4.2.1\n",
      "threadpoolctl                 2.2.0\n",
      "three-merge                   0.1.1\n",
      "tifffile                      2021.7.2\n",
      "tinycss                       0.4\n",
      "tldextract                    3.2.0\n",
      "toml                          0.10.2\n",
      "tomli                         2.0.1\n",
      "tomlkit                       0.11.1\n",
      "toolz                         0.11.2\n",
      "tornado                       6.1\n",
      "tqdm                          4.64.1\n",
      "traitlets                     5.1.1\n",
      "Twisted                       22.2.0\n",
      "twisted-iocpsupport           1.0.2\n",
      "typeguard                     2.13.3\n",
      "typing_extensions             4.3.0\n",
      "ujson                         5.4.0\n",
      "Unidecode                     1.2.0\n",
      "urllib3                       1.26.11\n",
      "visions                       0.7.5\n",
      "w3lib                         1.21.0\n",
      "watchdog                      2.1.6\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "websocket-client              0.58.0\n",
      "Werkzeug                      2.0.3\n",
      "wheel                         0.37.1\n",
      "widgetsnbextension            3.5.2\n",
      "win-inet-pton                 1.1.0\n",
      "win-unicode-console           0.5\n",
      "wincertstore                  0.2\n",
      "wrapt                         1.14.1\n",
      "xarray                        0.20.1\n",
      "xlrd                          2.0.1\n",
      "XlsxWriter                    3.0.3\n",
      "xlwings                       0.27.15\n",
      "yapf                          0.31.0\n",
      "ydata-profiling               4.1.2\n",
      "zict                          2.1.0\n",
      "zipp                          3.8.0\n",
      "zope.interface                5.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
